[
  {
    "name": "model",
    "label": "Model",
    "type": "select",
    "required": true,
    "default": "deepseek-chat",
    "help": "Choose the DeepSeek Model to be used.",
    "options": "rpc://getModels"
  },
  {
    "name": "messages",
    "label": "Messages",
    "type": "array",
    "required": true,
    "spec": [
      {
        "name": "role",
        "label": "Role",
        "type": "select",
        "required": true,
        "default": "user",
        "help": "Choose who 'speaks'",
        "options": [
          {
            "label": "User",
            "value": "user"
          },
          {
            "label": "System",
            "value": "system"
          },
          {
            "label": "Assistant",
            "value": "assistant"
          }
        ]
      },
      {
        "name": "content",
        "label": "Content",
        "type": "text",
        "required": true
      }
    ]
  },
  {
    "name": "temperature",
    "label": "Temperature",
    "type": "number",
    "default": 1,
    "validate": {"min":0,"max": 2},
    "help": "What sampling temperature to use, between 0 and 2. \nHigher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
  },
  {
    "name": "response_format",
    "label": "Response Format",
    "type": "collection",
    "spec": [
      {
        "name": "type",
        "label": "Type",
        "type": "select",
        "default": "text",
        "options": [
          {
            "label": "Text",
            "value": "text"
          },
          {
            "label": "JSON",
            "value": "json_object"
          }
        ]
      }
    ]
  },
  {
    "name": "max_tokens",
    "label": "Max Tokens",
    "type": "integer",
    "default": 4096,
    "validate": {"min":1,"max": 8192},
    "help": "Integer between 1 and 8192. The maximum number of tokens that can be generated in the chat completion."
  },
  {
    "name": "frequency_penalty",
    "label": "Frequency Penalty",
    "type": "number",
    "default": 0,
    "advanced": true,
    "validate": {"min":-2.0,"max": 2.0},
    "help": "Number between -2.0 and 2.0. \nPositive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
  },
  {
    "name": "presence_penalty",
    "label": "Presence Penalty",
    "type": "number",
    "default": 0,
    "advanced": true,
    "validate": {"min":-2.0,"max": 2.0},
    "help": "Number between -2.0 and 2.0. \nPositive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
  },
  {
    "name": "stop",
    "label": "Stop",
    "type": "array",
    "advanced": true,
    "help": "Up to 16 sequences where the API will stop generating further tokens.",
    "spec": [
      {
        "type": "text",
        "label": "Item"
      }
    ]
  },
  {
    "name": "stream",
    "label": "Stream",
    "type": "boolean",
    "default": false,
    "advanced": true,
    "required": true,
    "help": "If set, partial message deltas will be sent. \nTokens will be sent as data-only server-sent events (SSE) as they become available, with the stream terminated by a data: [DONE] message.",
    "nested": [
      {
        "name": "stream_options",
        "type": "collection",
        "label": "Stream Options",
        "help": "Options for streaming response.",
        "spec": [
          {
            "name": "include_usage",
            "type": "boolean",
            "default": false,
            "required": true,
            "label": "Include Usage?"
          }
        ]
      }
    ]
  },
  {
    "name": "top_p",
    "label": "Top P",
    "type": "number",
    "default": 1,
    "advanced": true,
    "help": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. \nSo 0.1 means only the tokens comprising the top 10% probability mass are considered."
  },
  {
    "name": "tools",
    "label": "Tools",
    "type": "array",
    "advanced": true,
    "help": "A list of tools the model may call. \nCurrently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.",
    "spec": [
      {
        "type": "select",
        "name": "type",
        "label": "Type",
        "help": "The type of the tool. Currently, only `function` is supported.",
        "options": [
          {
            "label": "Function",
            "value": "function",
            "nested": [
              {
                "type": "array",
                "required": true,
                "spec": [
                  {
                    "type": "text",
                    "label": "Description",
                    "name": "description",
                    "help": "A description of what the function does, used by the model to choose when and how to call the function."
                  },
                  {
                    "type": "text",
                    "required": true,
                    "name": "name",
                    "label": "Name",
                    "help": "The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."
                  },
                  {
                    "type": "collection",
                    "name": "parameters",
                    "label": "Parameters",
                    "help": "The parameters the functions accepts, described as a JSON Schema object. See the Function Calling Guide for examples, and the JSON Schema reference for documentation about the format.",
                    "spec": [
                      {
                        "name": "property name*",
                        "label": "Property Name*",
                        "type": "any",
                        "help": "The parameters the functions accepts, described as a JSON Schema object. See the Function Calling Guide for examples, and the JSON Schema reference for documentation about the format."
                      }
                    ]
                  }
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  {
    "type": "select",
    "label": "Tool Choice",
    "name": "tool_choice",
    "advanced": true,
    "help": "Controls which (if any) tool is called by the model.",
    "options": [
      {
        "label": "ChatCompletionToolChoice",
        "value": "ChatCompletionToolChoice",
        "nested": [
          {
            "name": "Possible values",
            "type": "select",
            "label": "Possible Values",
            "options": [
              {
                "label": "None",
                "value": "none"
              },
              {
                "label": "Auto",
                "value": "auto"
              },
              {
                "label": "Required",
                "value": "required"
              }
            ]
          }
        ]
      },
      {
        "label": "ChatCompletionNamedToolChoice",
        "value": "ChatCompletionNamedToolChoice",
        "nested": [
          {
            "type": "select",
            "label": "Type",
            "name": "type",
            "required": true,
            "help": "The type of the tool. Currently, only function is supported.",
            "options": [
              {
                "label": "Function",
                "value": "function",
                "nested": [
                  {
                    "type": "text",
                    "label": "Name",
                    "name": "name",
                    "help": "The name of the function to call."
                  }
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  {
    "type": "boolean",
    "name": "logprobs",
    "label": "Logprobs",
    "default": false,
    "required": true,
    "advanced": true,
    "help": "Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."
  },
  {
    "type": "integer",
    "name": "top_logprogbs",
    "label": "Top Logprobs",
    "advanced": true,
    "validate": {"min":0,"max": 20},
    "help": "An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. \n`logprobs` must be set to true if this parameter is used."
  }
]